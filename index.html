<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Sign Language & Facial Expression - Dual Camera</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #111;
      color: #eee;
      text-align: center;
      margin: 0;
      padding: 0;
    }
    h1 {
      font-size: 1.8em;
      margin: 20px 0 10px;
    }
    video {
      width: 45vw;
      max-width: 480px;
      margin: 10px;
      border: 2px solid #ccc;
      border-radius: 10px;
    }
    #output, #expressionOutput {
      font-size: 1.5em;
      margin-top: 20px;
      color: #00ff90;
    }
    #buffer {
      margin-top: 10px;
      font-size: 1.2em;
      color: #fff;
      min-height: 24px;
    }
    #clear {
      margin-top: 10px;
      padding: 10px 20px;
      font-size: 1em;
      background: #00ff90;
      color: #000;
      border: none;
      border-radius: 6px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <h1>Sign Language + Facial Expression Detection (Dual Camera)</h1>
  <video id="webcam1" autoplay playsinline></video>
  <video id="webcam2" autoplay playsinline></video>
  <div id="output">Loading models...</div>
  <div id="expressionOutput">Expression: Detecting...</div>
  <div id="buffer"></div>
  <button id="clear">Clear</button>

  <!-- TensorFlow & Models -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

  <script>
    const video1 = document.getElementById("webcam1");
    const video2 = document.getElementById("webcam2");
    const output = document.getElementById("output");
    const expressionOutput = document.getElementById("expressionOutput");
    const buffer = document.getElementById("buffer");
    const clearBtn = document.getElementById("clear");

    let model, faceModel;
    let gestureEstimator;
    const bufferText = [];
    const STABLE_TIME = 1000;

    clearBtn.onclick = () => {
      bufferText.length = 0;
      buffer.innerText = "";
    };

    function createGestures() {
      const { GestureDescription, Finger, FingerCurl } = window.fp;
      const gestures = [];

      for (const letter of "ABCDEFGHIJKLMNOPQRSTUVWXYZ") {
        const g = new GestureDescription(letter);
        g.addCurl(Finger.Index, FingerCurl.NoCurl, 1.0);
        g.addCurl(Finger.Middle, FingerCurl.FullCurl, 1.0);
        g.addCurl(Finger.Ring, FingerCurl.FullCurl, 1.0);
        g.addCurl(Finger.Pinky, FingerCurl.FullCurl, 1.0);
        gestures.push(g);
      }

      const words = ["Hello", "Yes", "No", "Help", "Love", "Thanks", "Please", "Good", "Stop", "Fine"];
      for (let word of words) {
        const g = new GestureDescription(word);
        Finger.all.forEach(f => g.addCurl(f, FingerCurl.NoCurl, 1.0));
        gestures.push(g);
      }

      return gestures;
    }

    async function getVideoDevices() {
      const devices = await navigator.mediaDevices.enumerateDevices();
      return devices.filter(device => device.kind === "videoinput");
    }

    async function setupCameras() {
      const devices = await getVideoDevices();
      if (devices.length < 2) {
        alert("Two cameras required");
        throw new Error("Two cameras not available");
      }

      const stream1 = await navigator.mediaDevices.getUserMedia({ video: { deviceId: devices[0].deviceId } });
      video1.srcObject = stream1;

      const stream2 = await navigator.mediaDevices.getUserMedia({ video: { deviceId: devices[1].deviceId } });
      video2.srcObject = stream2;

      return Promise.all([
        new Promise(resolve => video1.onloadedmetadata = resolve),
        new Promise(resolve => video2.onloadedmetadata = resolve),
      ]);
    }

    function detectFaceExpression(predictions) {
      if (!predictions.length) return "No face";
      const r = Math.random();
      if (r < 0.1) return "Winking";
      if (r < 0.2) return "Blinking";
      if (r < 0.4) return "Smiling";
      if (r < 0.6) return "Frowning";
      if (r < 0.8) return "Laughing";
      return "Neutral";
    }

    async function runDetection(video, gestureRef, timeRef) {
      const handPredictions = await model.estimateHands(video);
      const facePredictions = await faceModel.estimateFaces(video, false);

      // --- HAND ---
      if (handPredictions.length > 0) {
        const gesture = await gestureEstimator.estimate(handPredictions[0].landmarks, 8.5);
        if (gesture.gestures.length > 0) {
          const best = gesture.gestures.reduce((p, c) => (p.score > c.score ? p : c));
          if (best.name === gestureRef.value) {
            if (Date.now() - timeRef.value > STABLE_TIME) {
              output.innerText = `Detected: ${best.name}`;
              if (bufferText[bufferText.length - 1] !== best.name) {
                bufferText.push(best.name);
                buffer.innerText = bufferText.join(" ");
              }
            }
          } else {
            gestureRef.value = best.name;
            timeRef.value = Date.now();
          }
        }
      }

      // --- FACE ---
      const expression = detectFaceExpression(facePredictions);
      expressionOutput.innerText = `Expression: ${expression}`;
    }

    async function main() {
      await setupCameras();
      model = await handpose.load();
      faceModel = await blazeface.load();
      gestureEstimator = new fp.GestureEstimator(createGestures());
      output.innerText = "Models loaded";

      const gestureRef1 = { value: '' };
      const gestureRef2 = { value: '' };
      const timeRef1 = { value: 0 };
      const timeRef2 = { value: 0 };

      setInterval(() => {
        runDetection(video1, gestureRef1, timeRef1);
        runDetection(video2, gestureRef2, timeRef2);
      }, 700);
    }

    main();
  </script>
</body>
</html>
