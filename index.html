<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Sign Language to Text</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #111;
      color: #eee;
      text-align: center;
      margin: 0;
      padding: 0;
    }
    h1 {
      font-size: 1.8em;
      margin: 20px 0 10px;
    }
    video {
      width: 90vw;
      max-width: 640px;
      border: 2px solid #ccc;
      border-radius: 10px;
    }
    #output {
      font-size: 1.5em;
      margin-top: 20px;
      color: #00ff90;
      padding: 0 10px;
    }
    #buffer {
      margin-top: 10px;
      font-size: 1.2em;
      color: #fff;
      min-height: 24px;
    }
    button {
      margin: 10px 5px;
      padding: 10px 20px;
      font-size: 1em;
      background: #00ff90;
      color: #000;
      border: none;
      border-radius: 6px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <h1>Sign Language to Text</h1>
  <video id="webcam" autoplay playsinline></video>
  <div>
    <button id="switchCam">Switch Camera</button>
    <button id="clear">Clear</button>
  </div>
  <div id="output">Loading models...</div>
  <div id="buffer"></div>

  <!-- TensorFlow and Gesture Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>

  <script>
    const video = document.getElementById("webcam");
    const output = document.getElementById("output");
    const buffer = document.getElementById("buffer");
    const clearBtn = document.getElementById("clear");
    const switchCamBtn = document.getElementById("switchCam");

    let model;
    let gestureEstimator;
    let currentStream;
    let useFrontCamera = true;

    const bufferText = [];
    let lastGesture = '';
    let gestureStartTime = 0;
    const STABLE_TIME = 1000; // ms

    clearBtn.onclick = () => {
      bufferText.length = 0;
      buffer.innerText = "";
    };

    switchCamBtn.onclick = async () => {
      useFrontCamera = !useFrontCamera;
      await setupCamera();
    };

    function createGestures() {
      const { GestureDescription, Finger, FingerCurl } = window.fp;
      const gestures = [];

      // Add letters A-Z (simple example, not actual signs)
      for (const letter of "ABCDEFGHIJKLMNOPQRSTUVWXYZ") {
        const g = new GestureDescription(letter);
        g.addCurl(Finger.Index, FingerCurl.NoCurl, 1.0);
        g.addCurl(Finger.Middle, FingerCurl.FullCurl, 1.0);
        g.addCurl(Finger.Ring, FingerCurl.FullCurl, 1.0);
        g.addCurl(Finger.Pinky, FingerCurl.FullCurl, 1.0);
        gestures.push(g);
      }

      // Predefined common word gestures (simplified for demo)
      const words = ["Hello", "Yes", "No", "Help", "Love", "Thanks", "Please", "Good", "Stop", "Fine"];
      for (let word of words) {
        const g = new GestureDescription(word);
        Finger.all.forEach(f => g.addCurl(f, FingerCurl.NoCurl, 1.0));
        gestures.push(g);
      }

      return gestures;
    }

    async function setupCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }

      const constraints = {
        video: {
          facingMode: useFrontCamera ? "user" : "environment"
        },
        audio: false
      };

      currentStream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = currentStream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => resolve(video);
      });
    }

    async function main() {
      await setupCamera();
      model = await handpose.load();
      gestureEstimator = new fp.GestureEstimator(createGestures());
      output.innerText = "Models loaded";

      setInterval(async () => {
        const predictions = await model.estimateHands(video);

        if (predictions.length > 0) {
          const gesture = await gestureEstimator.estimate(predictions[0].landmarks, 8.5);
          if (gesture.gestures.length > 0) {
            const best = gesture.gestures.reduce((p, c) => (p.score > c.score ? p : c));
            if (best.name === lastGesture) {
              if (Date.now() - gestureStartTime > STABLE_TIME) {
                output.innerText = `Detected: ${best.name}`;
                if (bufferText[bufferText.length - 1] !== best.name) {
                  bufferText.push(best.name);
                  buffer.innerText = bufferText.join(" ");
                }
              }
            } else {
              lastGesture = best.name;
              gestureStartTime = Date.now();
            }
          }
        } else {
          output.innerText = "No hand detected";
          lastGesture = '';
          gestureStartTime = 0;
        }
      }, 700);
    }

    main();
  </script>
</body>
</html>
